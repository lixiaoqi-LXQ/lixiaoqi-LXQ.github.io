# HW2

## R4

不同意，因为P2P文件共享中，一个用户既可能作为客户端从其他用户处下载文件，也可以作为服务器端为他人提供文件片段。

## R5

主机地址(IP)+进程标识符(port number)

## R6

UDP。因为UDP不需要TCP协议的三次握手，能够尽快收到响应。

## R9

> 这里中文译本：强化UDP
> 
> 英文原版：*wants TCP to be enhanced*
> 
> 以英文版为准

SSL在应用层。

SSL的工作原理是：应用程序将明文传递给SSL套接字，SSL服务将其加密后传递给TCP套接字，经过网络传输，报文到达目标主机的TCP套接字，传递给SSL，解密后通过SSL套接字传递给接受进程。

因此程序研制者想利用SSL来强化TCP，需要向SSL发送明文数据，加密工作将有SSL服务完成。

## P1

1. 错。根据HTTP协议，服务器在一次应答中只返回一个响应报文，用户需要根据页面中的链接再次请求对象。

2. 对。因为主机可能一致，就可以通过持续连接的HTTP连续发送。

3. 错。同1，每个TCP报文只包含至多一个HTTP对象。

4. 错。Date：首部行指示服务器产生并发送响应报文的日期与时间。

5. 错。如HEAD方法对应的响应报文实体体为空。

## P5

1. 能。Date：Tue, 07 Mar 2008 12:39:45GMT

2. Last-Modified: Sat, 10 Dec2005 18:27:46 GMT

3. 3874

4. 前5个字节：`<!doc`
   
   同意持续连接

## R13

请求对象的总响应时间由三个部分组成：

- 局域网时延

- 接入时延

- 因特网时延

在接入链路带宽较小时，会导致接入链路成为速度瓶颈，使得接入时延较大。

如果网络核心的某些因素(如网络拥挤、物理链路限制等)导致因特网时延比较大，也会使总响应时间很大。

加入Web缓存就使得命中的请求只在局域网内部传输，局域网的链路带宽通常不会成为瓶颈；另一方面，未命中的对象请求只占一部分，这样得到的总响应时间期望值会远远小于原值，同时有利于减轻互联网负担。

理论上，Web缓存器能减少用户请求的所有对象的时延。因为Web缓存器的存在使得网络流量下降，所以即使是对于未命中的情况，响应时延也会有所下降。

## R16

```mermaid
flowchart LR
    A[User Agent for Alice]
    B[User Agent for Bob]
    M1[Mail Server 1]
    M2[Mail Server 2]
    A-->|HTTP|M1-->|SMTP|M2-->|POP3|B
```

## P9

> 这里中文译本：命中率0.4
> 
> 英文原版：*miss rate is 0.4*
> 
> 以英文版为准

| L(MB) | a(个/秒) | R(Mbps) | 平均因特网时延(s) | 平均接入时延                         |
| ----- | ------ | ------- | ---------- | ------------------------------ |
| 0.85  | 16     | 15      | 3          | $\triangle/(1-\triangle\beta)$ |

$\triangle=L/R=\frac{0.85MB}{15Mbps}\approx0.567s$

$\beta=16$

所以平均接入时延是$\triangle/(1-\triangle\beta)=\frac{0.567}{1-0.567\times16}\approx0.607s$

因此总的平均响应时间为：$3s+0.607s=3.607s$

安装了Web缓存器后：

- 命中部分，$delay=0s$

- 未命中部分，重新计算平均接入时延
  
  $\triangle/(1-\triangle\beta')=\frac{0.567}{1-0.567\times16\times0.4}\approx0.089s$
  
  此时平均响应时间为：$3s+0.089s=3.089s$

故总的响应时间变为：$0s\times0.6+0.124s\times0.4\approx1.24s$

## P10

因为是只有10m长的短链路，所以传播时延可以忽略不计，只考虑传输时延。

**持续连接**

- 第一次接收数据：$200bit/150bps\times3+10^5bit/150bps$

- 使用同一连接先后接收另外10个引用：$10\times(200bit/150bps+10^5bit/150bps)$

- 求和得到结果：$7350.7s$

**非持续连接**

- 第一次接收数据：$200bit/150bps\times3+10^5bit/150bps$

- 10个并发的过程(建立连接+控制报文+接收文件)，此时带宽被共享，只剩$15bps$
  
  $200bit/15bps\times3+10^5bit/15bps$

- 求和得到结果：$7377.3s$

综上分析可知：

1. 并行下载一来不削减链路负载，二来链路带宽被均匀共享，所以并没有意义。

2. 持续连接带来的收益主要是更少的连接建立时期的控制报文，但是由于此案例中，控制报文的长度和数据报文相比微不足道，所以持续连接并没有带来很大增益。

## R19

可以。邮件服务器的别名记录，具有MX类型。

## R22

其他对等方选择传输对象时，会周期性地选择随机邻居发送对象，这样Alice就有机会获得初始文件块。

## R23

覆盖网络是参与进P2P文件共享系统中的节点以及节点间的逻辑连边组成的。如果两个节点之间存在TCP连接，则认为两个节点间存在一条边。覆盖网络不包含路由器。

## P11

1. 可以。因为总并行量为4个其他用户的非并行连接+Bob的若干并行连接，即并行连接为Bob争取了更大的带宽。

2. 由于并行连接均等共享带宽，所以提高并行量可以为自己争取更大的带宽，因此Bob保持并行连接将为他避免带宽被其他用户抢占过多。

## P22

$$
D_{cs}=max\{\frac {NF}{u_s},\frac F{d_{min}}\}\\
D_{p2p}=max\{\frac {F}{u_s},\frac F{d_{min}},\frac{NF}{u_s+\sum d_i}\}\\
$$

| 组合(N,u)         | c/s(s)   | p2p(s)  |
| --------------- | -------- | ------- |
| (10, 300kbps)   | 7680.0   | 7680.0  |
| (10, 700kbps)   | 7680.0   | 7680.0  |
| (10, 2Mbps)     | 7680.0   | 7680.0  |
| (100, 300kbps)  | 51200.0  | 25903.6 |
| (100, 700kbps)  | 51200.0  | 15616.2 |
| (100, 2Mbps)    | 51200.0  | 7680.0  |
| (1000, 300kbps) | 512000.0 | 47558.8 |
| (1000, 700kbps) | 512000.0 | 21524.9 |
| (1000, 2Mbps)   | 512000.0 | 7680.0  |

## R26

TCP服务器需要一个欢迎socket，用来与尝试连接的客户端主机进行握手，在连入一个用户后为这个用户的服务单独分配一个socket，所以需要两个socket。

而UDP对于所有连接都只使用一个socket通信。

当TCP服务器同时为n条连接服务时，需要n+1个socket。

## P23

1. 服务器以$\frac{u_s}{N}$的速率向$N$个对等方同时发送

2. 服务器以$d_{min}$的速率向$N$个对等方同时发送

3. 一般情况下，服务器可以向多个对等方并行传输，带宽被均分。最佳情况下，网络核心的带宽足够大，且收发两侧都竭尽全力。
   
此时：
   
- 若服务器上载带宽充足，则用户下载带宽为瓶颈，耗用时间$\frac F{d_{min}}$
   
- 若用户数量庞大，使得服务器上载带宽成为瓶颈，耗用时间$\frac{NF}{u_s}$
   
   文件分发分为上传和下载，都要完成，所以取两者最大值作为最小分发时间。

## P24

1. $u_s\le(u_s+u_1+\dots+u_N)/N\Rightarrow(N-1)u_s\le(u_1+\dots+u_N)$
   
   记$u=u_1+\dots+u_N$
   
   服务器将文件划分为$N$个小块，每块的大小是$s_i=\frac{u_i}u\cdot F$。第$i$块文件以速率$u_{si}=\frac{u_i}u\cdot u_s$分发给第$i$个节点。这时服务器的上载链路满载
   
   当第$i$个节点接收到文件开始，也以速率$s_i$向其他$N-1$个节点转发文件块，第$i$个节点的总上载速度是$(N-1)s_i=\frac{(N-1)u_s}{u}\cdot u_i\le u_i$
   
   当服务器分发完文件时，各对等节点也已经将自身接收到的文件块共享给其他对等节点。所以此时分发时间是$\frac F{u_s}$

2. 类似上述，改为：
   
    - 服务器将文件拆分为$N+1$个块，比例是：$\frac{u_1}{N-1}:\frac{u_2}{N-1}:\dots:\frac{u_N}{N-1}:(u_s-\frac u{N-1})/N$
      
      并以速率$u_{si}=\frac{u_i}{N-1}$向第$i$个对等节点分发文件块$i$，以速率$u_{sN+1}=(u_s-\frac u{N-1})/N$向全部$N$个对等节点发送文件块$N+1$
      
      则总上载速率$u_{s1}+\dots+u_{sN}+N\cdot u_{sN+1}= u_s$
   
    - 第$i$个节点接收到文件块，立刻以速率$\frac{u_i}{N-1}$向其余$N-1$个节点转发，其上载链路正好满载
   
   如此，第$i$个节点接受文件的速度之和是：
   
   $u_{si}+u_{sN+1}+\sum_{j\neq i}\frac{u_j}{N-1}=\frac{u+u_s}N$
   
   所以总的分发时间是$F/\frac{u+u_s}N=\frac{NF}{u+u_s}$

3. 综合以上两种情况，分别对应服务器上载链路带宽不足和整个网络满载的情况，所以得到最小分发时间取二者最大值。
